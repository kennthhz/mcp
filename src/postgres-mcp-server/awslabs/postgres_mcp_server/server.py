# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""awslabs postgres MCP Server implementation."""

import argparse
import asyncio
import json
import sys
import threading
import traceback
import boto3

from awslabs.postgres_mcp_server.connection.db_connection_map import DBConnectionMap, ConnectionMethod
from awslabs.postgres_mcp_server.connection.rds_api_connection import RDSDataAPIConnection
from awslabs.postgres_mcp_server.connection.cp_api_connection import internal_get_cluster_properties, internal_create_serverless_cluster, internal_create_express_cluster, internal_delete_cluster, get_rds_cluster_and_secret_arn, internal_delete_express_cluster
from awslabs.postgres_mcp_server.connection.psycopg_pool_connection import PsycopgPoolConnection
from awslabs.postgres_mcp_server.connection.abstract_db_connection import AbstractDBConnection
from awslabs.postgres_mcp_server.mutable_sql_detector import (
    check_sql_injection_risk,
    detect_mutating_keywords,
)
from botocore.exceptions import BotoCoreError, ClientError
from loguru import logger
from mcp.server.fastmcp import Context, FastMCP
from pydantic import Field
from typing import Annotated, Any, Dict, List, Optional, Tuple
from datetime import datetime


db_connection_map = DBConnectionMap()
async_job_status: Dict[str, dict] = {}
async_job_status_lock = threading.Lock()
client_error_code_key = 'run_query ClientError code'
unexpected_error_key = 'run_query unexpected error'
write_query_prohibited_key = 'Your MCP tool only allows readonly query. If you want to write, change the MCP configuration per README.md'
query_comment_prohibited_key = 'The comment in query is prohibited because of injection risk'
query_injection_risk_key = 'Your query contains risky injection patterns'
readonly_query = True

class DummyCtx:
    """A dummy context class for error handling in MCP tools."""

    async def error(self, message):
        """Raise a runtime error with the given message.

        Args:
            message: The error message to include in the runtime error
        """
        # Do nothing
        pass


def extract_cell(cell: dict):
    """Extracts the scalar or array value from a single cell."""
    if cell.get('isNull'):
        return None
    for key in (
        'stringValue',
        'longValue',
        'doubleValue',
        'booleanValue',
        'blobValue',
        'arrayValue',
    ):
        if key in cell:
            return cell[key]
    return None


def parse_execute_response(response: dict) -> list[dict]:
    """Convert RDS Data API execute_statement response to list of rows."""
    columns = [col['name'] for col in response.get('columnMetadata', [])]
    records = []

    for row in response.get('records', []):
        row_data = {col: extract_cell(cell) for col, cell in zip(columns, row)}
        records.append(row_data)

    return records


mcp = FastMCP(
    'pg-mcp MCP server. This is the starting point for all solutions created',
    dependencies=[
        'loguru',
    ])


@mcp.tool(name='run_query', description='Run a SQL query against PostgreSQL')
async def run_query(
    sql: Annotated[str, Field(description='The SQL query to run')],
    ctx: Context,
    connection_method: Annotated[ConnectionMethod, Field(description='connection method')],
    cluster_identifier: Annotated[str, Field(description='Cluster identifier')],
    db_endpoint: Annotated[str, Field(description='database endpoint')],
    database: Annotated[str, Field(description='database name')],
    query_parameters: Annotated[
        Optional[List[Dict[str, Any]]], Field(description='Parameters for the SQL query')
    ] = None) -> list[dict]:  # type: ignore

    """Run a SQL query against PostgreSQL.

    Args:
        sql: The sql statement to run
        ctx: MCP context for logging and state management
        connection_method: connection method
        cluster_identifier: Cluster identifier
        db_endpoint: database endpoint
        database: database name
        query_parameters: Parameters for the SQL query

    Returns:
        List of dictionary that contains query response rows
    """
    global client_error_code_key
    global unexpected_error_key
    global write_query_prohibited_key
    global db_connection_map

    logger.info(f'Entered run_query with '
             f'method:{connection_method}, cluster_identifier:{cluster_identifier}, '
             f'db_endpoint:{db_endpoint}, database:{database}, '
             f'sql:{sql}')

    db_connection = db_connection_map.get(
        method=connection_method, 
        cluster_identifier=cluster_identifier, 
        db_endpoint=db_endpoint, 
        database=database)
    if not db_connection:
        err = (f'No database connection available for method:{connection_method},',
               f' cluster_identifier:{cluster_identifier}, db_endpoint:{db_endpoint}, database:{database}')
        logger.error(err)
        await ctx.error(err)
        return [{'error': err}]

    if db_connection.readonly_query:
        matches = detect_mutating_keywords(sql)
        if (bool)(matches):
            logger.info(
                (f'query is rejected because current setting only allows readonly query.'
                 f'detected keywords: {matches}, SQL query: {sql}'))
            await ctx.error(write_query_prohibited_key)
            return [{'error': write_query_prohibited_key}]

    issues = check_sql_injection_risk(sql)
    if issues:
        logger.info(f'query is rejected because it contains risky SQL pattern, SQL query: {sql}, reasons: {issues}')
        await ctx.error(str({'message': 'Query parameter contains suspicious pattern', 'details': issues}))
        return [{'error': query_injection_risk_key}]

    try:
        logger.info((
            f"run_query: sql:{sql} method:{connection_method}, "
            f"cluster_identifier:{cluster_identifier} database:{database} "
            f"db_endpoint:{db_endpoint} "
            f"readonly:{db_connection.readonly_query} query_parameters:{query_parameters}"))

        response = await db_connection.execute_query(sql, query_parameters)

        logger.success(f'run_query successfully executed query:{sql}')
        return parse_execute_response(response)
    except ClientError as e:
        logger.exception(client_error_code_key)
        await ctx.error(
            str({'code': e.response['Error']['Code'], 'message': e.response['Error']['Message']})
        )
        return [{'error': client_error_code_key}]
    except Exception as e:
        logger.exception(unexpected_error_key)
        error_details = f'{type(e).__name__}: {str(e)}'
        await ctx.error(str({'message': error_details}))
        return [{'error': unexpected_error_key}]


@mcp.tool(
    name='get_table_schema',
    description='Fetch table columns and comments from Postgres')
async def get_table_schema(
    connection_method: Annotated[ConnectionMethod, Field(description='connection method')],
    cluster_identifier: Annotated[str, Field(description='Cluster identifier')],
    db_endpoint: Annotated[str, Field(description='database endpoint')],
    database: Annotated[str, Field(description='database name')],
    table_name: Annotated[str, Field(description='name of the table')],
    ctx: Context) -> list[dict]:
    """Get a table's schema information given the table name.

    Args:
        connection_method: connection method
        cluster_identifier: Cluster identifier
        db_endpoint: database endpoint
        database: database name
        table_name: name of the table
        ctx: MCP context for logging and state management

    Returns:
        List of dictionary that contains query response rows
    """
    logger.info((f"Entered get_table_schema: table_name:{table_name} connection_method:{connection_method}, "
                f"cluster_identifier:{cluster_identifier}, db_endpoint:{db_endpoint}, database:{database}"))

    sql = """
        SELECT
            a.attname AS column_name,
            pg_catalog.format_type(a.atttypid, a.atttypmod) AS data_type,
            col_description(a.attrelid, a.attnum) AS column_comment
        FROM
            pg_attribute a
        WHERE
            a.attrelid = to_regclass(:table_name)
            AND a.attnum > 0
            AND NOT a.attisdropped
        ORDER BY a.attnum
    """

    params = [{'name': 'table_name', 'value': {'stringValue': table_name}}]

    return await run_query(sql=sql, ctx=ctx, connection_method = connection_method,
                           cluster_identifier=cluster_identifier, db_endpoint= db_endpoint,
                           database=database, query_parameters=params)


@mcp.tool(
    name='connect_to_database',
    description='Connect to a specific database and save the connection internally')
def connect_to_database(
    region: Annotated[str, Field(description='region')],
    cluster_identifier: Annotated[str, Field(description='cluster identifier')],
    db_endpoint: Annotated[str, Field(description='database endpoint')],
    port: Annotated[int, Field(description='Postgres port')],
    database: Annotated[str, Field(description='database name')],
    with_express_configuration: Annotated[bool, Field(description='with express configuration')] = False) -> str:
        
    """Connect to a specific database save the connection internally

    Args:
        region: region
        cluster_identifier: cluster identifier
        db_endpoint: database endpoint
        database: database name
        with_express_configuration: if the database is associated express configuration
    """

    try:
        db_connection, llm_response = internal_connect_to_database(
            region=region,
            cluster_identifier=cluster_identifier,
            db_endpoint=db_endpoint,
            port=port,
            database=database,
            with_express_configuration=with_express_configuration
        )

        return str(llm_response)

    except Exception as e:
        logger.error(f"internal_create_express_cluster failed with error: {str(e)}")
        trace_msg = traceback.format_exc()
        logger.error(f"Trace:{trace_msg}")
        llm_response = {
                "status":"Failed",
                "error":str(e)
            }
        return json.dumps(llm_response, indent=2) 

    
@mcp.tool(
    name='is_database_connected',
    description='Check if a connection has been established')
def is_database_connected(
    cluster_identifier: Annotated[str, Field(description='cluster identifier')],
    db_endpoint: Annotated[str, Field(description='database endpoint')] = '',
    database: Annotated[str, Field(description='database name')] = 'postgres')->bool:

    """Check if a connection has been established

    Args:
        cluster_identifier: cluster identifier
        db_endpoint: database endpoint
        database: database name

    Returns:
        result in boolean
    """

    global db_connection_map
    if db_connection_map.get(ConnectionMethod.RDS_API, cluster_identifier, db_endpoint, database):
        return True
    
    if db_connection_map.get(ConnectionMethod.PG_WIRE_PROTOCOL, cluster_identifier, db_endpoint, database):
        return True

    return False


@mcp.tool(
    name='delete_express_cluster',
    description='Delete an express Aurora Postgres cluster')
def delete_express_cluster(
    cluster_identifier: Annotated[str, Field(description='cluster identifier')]) -> None:

    """Delete an express Aurora Postgres cluster
    Args:
        cluster_identifier: cluster identifier
    """

    internal_delete_express_cluster(cluster_identifier)

@mcp.tool(
    name='create_cluster',
    description='Create an RDS/Aurora cluster')
def create_cluster(
    region: Annotated[str, Field(description='region')],
    cluster_identifier: Annotated[str, Field(description='cluster identifier')],
    database: Annotated[str, Field(description='default database name')] = 'postgres',
    engine_version: Annotated[str, Field(description='engine version')] = '17.5',
    with_express_configuration: Annotated[bool, Field(description='with express configuration')] = False) -> str:

    """Create an RDS/Aurora cluster

    Args:
        region: region
        cluster_identifier: cluster identifier
        database: database name
        engine_version: engine version
        with_express_configuration: create the cluster with express configuration

    Returns:
        result
    """

    logger.info(f'Entered create_cluster with region:{region}, '
                f'cluster_identifier:{cluster_identifier} '
                f'database:{database} '
                f'engine_version:{engine_version} '
                f'with_express_configuration:{with_express_configuration}')

    if with_express_configuration:
        response = internal_create_express_cluster(cluster_identifier)
        internal_connect_to_database(
            region = region,
            cluster_identifier=cluster_identifier,
            db_endpoint=response['Endpoint'],
            port=5432,
            database=database,
            with_express_configuration=with_express_configuration)

        result = {
            "status":"Completed",
            "cluster_identifier": cluster_identifier,
            "message":"cluster creation completed successfully"
        }

        return json.dumps(result, indent=2)


    job_id = f"create-cluster-{cluster_identifier}-{datetime.now().isoformat(timespec='milliseconds')}"

    try:
        async_job_status_lock.acquire()
        async_job_status[job_id] = {"state":"pending", "result":None}
    finally:
        async_job_status_lock.release()

    t = threading.Thread(
        target=create_cluster_worker,
        args=(job_id, region, cluster_identifier, engine_version, database),
        daemon=False,
    )
    t.start()

    logger.info(f"start_create_cluster_job return with job_id:{job_id}"
                f"region:{region} cluster_identifier:{cluster_identifier} database:{database} "
                f"engine_version:{engine_version} with_express_configuration: {with_express_configuration}")
    
    result = {
            "status": "Pending",
            "message": "cluster creation started",
            "job_id": job_id,
            "cluster_identifier": cluster_identifier,
            "check_status_tool": "get_job_status",
            "next_action": f"Use get_job_status(job_id='{job_id}') to get results"
    }

    return json.dumps(result, indent=2)

@mcp.tool(
    name='delete_cluster',
    description='Delete an RDS/Aurora cluster')
def delete_cluster(
    region: Annotated[str, Field(description='region')],
    cluster_identifier: Annotated[str, Field(description='cluster identifier')],
    with_express_configuration: Annotated[bool, Field(description='with express configuration')] = False)-> str:

    """Start a background job to delete a RDS or Aurora Postgres cluster and its instances

    Args:
        region: region
        cluster_identifier: cluster identifier
        with_express_configuration: the cluster is associated with express configuration

    Returns:
         result
    """
    global async_job_status
    global async_job_status_lock

    job_id = f"delete-cluster-{cluster_identifier}-{datetime.now().isoformat(timespec='milliseconds')}"

    try:
        async_job_status_lock.acquire()
        async_job_status[job_id] = {"state":"pending", "result":None}
    finally:
        async_job_status_lock.release()

    t = threading.Thread(
        target=delete_cluster_worker,
        args=(job_id, region, cluster_identifier, with_express_configuration),
        daemon=False,
    )
    t.start()

    logger.info(f"start_delete_cluster_job return with job_id:{job_id}"
                f"region:{region} cluster_identifier:{cluster_identifier}, with_express_configuration:{with_express_configuration}")
    result = {
            "status": "Pending",
            "message": "cluster deletion started",
            "job_id": job_id,
            "check_status_tool": "get_job_status",
            "next_action": f"Use get_job_status(job_id='{job_id}') to get results"
    }

    return json.dumps(result, indent=2)


@mcp.tool(
    name='get_job_status',
    description='get background job status')
def get_job_status(job_id: str) -> dict:

    """Get background job status

    Args:
        job_id: job id
    Returns:
        job status
    """
    global async_job_status
    global async_job_status_lock

    try:
        async_job_status_lock.acquire()
        return async_job_status.get(job_id, {"state": "not_found"})
    finally:
        async_job_status_lock.release()

def create_cluster_worker(
        job_id:str, 
        region:str, 
        cluster_identifier:str, 
        engine_version:str, 
        database:str):
    global db_connection_map
    global async_job_status
    global async_job_status_lock
    global readonly_query

    try:
        cluster_result = internal_create_serverless_cluster(
            region = region,
            cluster_identifier = cluster_identifier,
            engine_version = engine_version,
            database_name = database)
        
        internal_connect_to_database(
            region = region,
            cluster_identifier=cluster_identifier,
            db_endpoint=cluster_result["Endpoint"],
            port=5432,
            database=database)
    
        try:
            async_job_status_lock.acquire()
            async_job_status[job_id]["state"] = "succeeded"
        finally:
            async_job_status_lock.release()
    except Exception as e:
        logger.error(f"create_cluster_worker failed with {e}")
        try:
            async_job_status_lock.acquire()
            async_job_status[job_id]["state"] = "failed"
            async_job_status[job_id]["result"] = str(e)
        finally:
            async_job_status_lock.release()

def delete_cluster_worker(job_id:str, region:str, cluster_identifier:str):
    try:
        internal_delete_cluster(region, cluster_identifier)
        try:
            async_job_status_lock.acquire()
            async_job_status[job_id]["state"] = "succeeded"
        finally:
            async_job_status_lock.release()
    except Exception as e:
        try:
            async_job_status_lock.acquire()
            async_job_status[job_id]["state"] = "failed"
            async_job_status[job_id]["result"] = str(e)
        finally:
            async_job_status_lock.release()

def internal_connect_to_database(
    region: Annotated[str, Field(description='region')],
    cluster_identifier: Annotated[str, Field(description='cluster identifier')],
    db_endpoint: Annotated[str, Field(description='database endpoint')],
    port: Annotated[int, Field(description='Postgres port')],
    database: Annotated[str, Field(description='database name')] = 'postgres',
    with_express_configuration: Annotated[bool, Field(description='with express configuration')] = False) -> Tuple:
        
    """Connect to a specific database save the connection internally

    Args:
        region: region
        cluster_identifier: cluster identifier
        db_endpoint: database endpoint
        database: database name
        with_express_configuration: if the database is associated express configuration
    """

    global db_connection_map
    global readonly_query

    logger.info(f'Enter internal_connect_to_database\n'
                f'region:{region}\n'
                f'cluster_identifier:{cluster_identifier}\n'
                f'db_endpoint:{db_endpoint}\n'
                f'database:{database}\n'
                f'readonly_query:{readonly_query}\n'
                f'with_express_configuration:{with_express_configuration}')

    if not region:
        raise ValueError("region can't be none or empty")
    
    if not cluster_identifier:
        raise ValueError("cluster_identifier can't be none or empty")
    
    connection_method = ConnectionMethod.RDS_API
    existing_conn = db_connection_map.get(ConnectionMethod.RDS_API, cluster_identifier, db_endpoint, database)
    if not existing_conn:
        existing_conn = db_connection_map.get(ConnectionMethod.PG_WIRE_PROTOCOL, cluster_identifier, db_endpoint, database)   
        connection_method = ConnectionMethod.PG_WIRE_PROTOCOL


    if existing_conn and (not existing_conn.is_expired()):
        llm_response = json.dumps({
                "connection_method": connection_method,
                "cluster_identifier": cluster_identifier,
                "db_endpoint": db_endpoint,
                "database" : database
            }, indent=2, default=str)
        return (existing_conn, llm_response)
    
    enable_data_api:bool = False
    masteruser:str = ''
    cluster_arn:str = ''
    secret_arn:str = ''
    with_express_config:bool = False
    
    properties = internal_get_cluster_properties(
        cluster_identifier=cluster_identifier, 
        region=region, 
        with_express_configuration=with_express_configuration)

    enable_data_api = properties.get("HttpEndpointEnabled", False)
    masteruser = properties.get("MasterUsername", '')
    cluster_arn = properties.get("DBClusterArn", '')
    secret_arn = properties.get("MasterUserSecret", {}).get("SecretArn")
    db_endpoint = properties.get("Endpoint", '')
    
    logger.info(f"enable_data_api:{enable_data_api} "
                f"masteruser:{masteruser} "
                f"cluster_arn:{cluster_arn} "
                f"secret_arn:{secret_arn} "
                f"with_express_config:{with_express_config} ")
    
    db_connection = None
    if with_express_configuration:
        rds_client = boto3.client('rds', region_name=region)
        token = rds_client.generate_db_auth_token(
            DBHostname=db_endpoint,
            Port=port,
            DBUsername=masteruser,
            Region=region
        )

        db_connection = PsycopgPoolConnection(
            host=db_endpoint,
            port=port,
            database=database,
            readonly=readonly_query,
            secret_arn='',
            db_user=masteruser,
            iam_auth_token=token,
            region=region)
        connection_method = ConnectionMethod.PG_WIRE_PROTOCOL

    elif enable_data_api and secret_arn:
        db_connection = RDSDataAPIConnection(
            cluster_arn=cluster_arn,
            secret_arn=str(secret_arn),
            database=database,
            region=region,
            readonly=readonly_query)
        connection_method = ConnectionMethod.RDS_API

    elif db_endpoint and secret_arn:
        db_connection = PsycopgPoolConnection(
            host=db_endpoint,
            port=port,
            database=database,
            readonly=readonly_query,
            secret_arn=secret_arn,
            db_user='',
            iam_auth_token='',
            region=region)
        connection_method = ConnectionMethod.PG_WIRE_PROTOCOL
    
    if db_connection:
        db_connection_map.set(connection_method, cluster_identifier, db_endpoint, database, db_connection)
        llm_response = json.dumps({
                "connection_method": connection_method,
                "cluster_identifier": cluster_identifier,
                "db_endpoint": db_endpoint,
                "database" : database
            }, indent=2, default=str)
        
        return (db_connection, llm_response)


    raise ValueError("Can't create connection becaseu of parameter")

def main():

    """
    Main entry point for the MCP server application.
    
    Runs the MCP server with CLI argument support for PostgreSQL connections.
    """
    global db_connection_map
    global readonly_query

    parser = argparse.ArgumentParser(
        description='An AWS Labs Model Context Protocol (MCP) server for postgres'
    )

    parser.add_argument('--connection_method', help='Connection method to the database. It can be RDS_API or PG_WIRE_PROTOCOL)')
    parser.add_argument('--authentication_method', help='Authentication method to connect to the database. It can be Postgres or IAM)')
    parser.add_argument('--resource_arn', help='ARN of the RDS cluster (for RDS Data API)')
    parser.add_argument('--hostname', help='Database hostname (for direct PostgreSQL connection)')
    parser.add_argument('--region', help='AWS region')
    parser.add_argument('--readonly', action='store_true', help='Enforce readonly SQL statements')

    # secret_arn is only applicable if auth method is Postgres which means PG user and password. 
    # It doesn't apply to IAM auth who token is dyanmically generated
    parser.add_argument('--secret_arn', help='ARN of the Secrets Manager secret for database credentials')

    # If database is not set for connection_method = PG_WIRE_PROTOCOL, MCP will not make a connection during start up.
    # Instead, the user should make connection with MCP tool calls
    parser.add_argument('--database', help='Database name')
    parser.add_argument('--port', type=int, default=5432, help='Database port (default: 5432)')

    args = parser.parse_args()

    if args.connection_method == 'PG_WIRE_PROTOCOL' and (not args.hostname):
        parser.error('You must set --hostname when connection method is PG_WIRE_PROTOCOL')

    if args.connection_method == 'RDS_API' and args.hostname:
        parser.error('You must not set --hostname when connection method is RDS_API')

    if args.authentication_method == 'IAM' and args.connection_method == 'RDS_API':
        parser.error('IAM authentication is not supported for connection method of RDS_API')

    readonly_query = args.readonly

    logger.info(f"MCP configuration:\n"
                f"resouce_arn:{args.resource_arn}\n"
                f"hostname:{args.hostname}\n"
                f"region:{args.region}\n"
                f"readonly:{readonly_query}\n"
                f"database:{args.database}\n"
                f"port:{args.port}\n")

    try:
        if args.resource_arn:
            # Create the appropriate database connection based on the provided parameters
            db_connection: Optional[AbstractDBConnection] = None
            connection_method: ConnectionMethod = ConnectionMethod.RDS_API

            cluster_identifier = args.resource_arn.split(":")[-1]
            db_connection, llm_response = internal_connect_to_database(
                region=args.region,
                cluster_identifier=cluster_identifier,
                db_endpoint=args.hostname,
                port=args.port,
                database=args.database)

            # Test database connection
            if db_connection:
                ctx = DummyCtx()
                response = asyncio.run(run_query('SELECT 1', ctx, 
                    connection_method, cluster_identifier, args.hostname, args.database))
                if (
                    isinstance(response, list)
                    and len(response) == 1
                    and isinstance(response[0], dict)
                    and 'error' in response[0]
                ):
                    logger.error('Failed to validate database connection to Postgres. Exit the MCP server')
                    sys.exit(1)
                else:
                    logger.success('Successfully validated database connection to Postgres')


        logger.info('Postgres MCP server started')
        mcp.run()
        logger.info('Postgres MCP server stopped')
    finally:
        db_connection_map.close_all()

if __name__ == '__main__':
    main()
